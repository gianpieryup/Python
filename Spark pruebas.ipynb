{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fe3d548",
   "metadata": {},
   "source": [
    "# Campo Lista Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bf6279",
   "metadata": {},
   "source": [
    "Para usar pyspark en una notebook debemos tener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8cdceee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9bdfe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69108065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos la session de Spark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cde081",
   "metadata": {},
   "source": [
    "### Ejemplo\n",
    "\n",
    "Les comparto un campo de ejemplo de una tabla con este tipo de estructura, como veran es un string plano\n",
    "\n",
    "```sql\n",
    "SELECT actividad_regimen_general\n",
    "FROM xxxx\n",
    "WHERE cuit_cuil = '20008688657'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e71f39ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Some of types cannot be determined after inferring",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2904/3537452960.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#jsonString = 'Null'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_json\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"2023-05-01\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"actividad_regimen_general\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"partition_date\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf_json\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    892\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m             )\n\u001b[1;32m--> 894\u001b[1;33m         return self._create_dataframe(\n\u001b[0m\u001b[0;32m    895\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverifySchema\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    896\u001b[0m         )\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    934\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 936\u001b[1;33m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    937\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[1;34m(self, data, schema)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 631\u001b[1;33m             \u001b[0mstruct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inferSchemaFromList\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    632\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m             \u001b[0mtupled_data\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_inferSchemaFromList\u001b[1;34m(self, data, names)\u001b[0m\n\u001b[0;32m    520\u001b[0m         )\n\u001b[0;32m    521\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 522\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Some of types cannot be determined after inferring\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    523\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Some of types cannot be determined after inferring"
     ]
    }
   ],
   "source": [
    "#jsonString = 'Null'\n",
    "df_json=spark.createDataFrame([(None, \"2023-05-01\")],[\"actividad_regimen_general\",\"partition_date\"])\n",
    "df_json.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de898d7",
   "metadata": {},
   "source": [
    "Bueno, ya tenemos la columna en un dataframe (este paso anterior no sera necesario porque ustedes se conectan al lago)\n",
    "\n",
    "Ahora vamos a pasarle la estructura que deberia tener este texto plano, que es **como queremos que se comporte**\n",
    "\n",
    "Al verlo intuimos lo siguiente: Una lista de estructuras, la cual se escribe en SPARK de la siguiente forma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a08b606",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve 'CASE WHEN (actividad_regimen_general = 'Null') THEN 'no' ELSE from_json(actividad_regimen_general) END' due to data type mismatch: THEN and ELSE expressions should all be same type or coercible to a common type, got CASE WHEN ... THEN string ELSE array<struct<descripcionActividad:string,idActividad:string,nomenclador:string,orden:string,periodo:string>> END;\n'Project [actividad_regimen_general#0, partition_date#1, CASE WHEN (actividad_regimen_general#0 = Null) THEN no ELSE from_json(ArrayType(StructType(StructField(descripcionActividad,StringType,true),StructField(idActividad,StringType,true),StructField(nomenclador,StringType,true),StructField(orden,StringType,true),StructField(periodo,StringType,true)),true), actividad_regimen_general#0, Some(America/Buenos_Aires)) END AS act#85]\n+- LogicalRDD [actividad_regimen_general#0, partition_date#1], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2904/3624934153.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#  creamos una nueva columna: withColumn(\"un alias\", from_json(\"la columna a parsear\", el_schema_creado))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m mapped_df = df_json.withColumn(\"act\", when(df_json.actividad_regimen_general == 'Null', 'no')\n\u001b[0m\u001b[0;32m     13\u001b[0m                                .otherwise(from_json(\"actividad_regimen_general\", schema))   )   \n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[1;34m(self, colName, col)\u001b[0m\n\u001b[0;32m   3034\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3035\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"col should be Column\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3036\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3037\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3038\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DataFrame\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: cannot resolve 'CASE WHEN (actividad_regimen_general = 'Null') THEN 'no' ELSE from_json(actividad_regimen_general) END' due to data type mismatch: THEN and ELSE expressions should all be same type or coercible to a common type, got CASE WHEN ... THEN string ELSE array<struct<descripcionActividad:string,idActividad:string,nomenclador:string,orden:string,periodo:string>> END;\n'Project [actividad_regimen_general#0, partition_date#1, CASE WHEN (actividad_regimen_general#0 = Null) THEN no ELSE from_json(ArrayType(StructType(StructField(descripcionActividad,StringType,true),StructField(idActividad,StringType,true),StructField(nomenclador,StringType,true),StructField(orden,StringType,true),StructField(periodo,StringType,true)),true), actividad_regimen_general#0, Some(America/Buenos_Aires)) END AS act#85]\n+- LogicalRDD [actividad_regimen_general#0, partition_date#1], false\n"
     ]
    }
   ],
   "source": [
    "schema = ArrayType(StructType(\n",
    "      [\n",
    "        StructField('descripcionActividad',  StringType(), True ),\n",
    "        StructField('idActividad', StringType(), True),\n",
    "        StructField('nomenclador', StringType(), True),\n",
    "        StructField('orden', StringType(), True),\n",
    "        StructField('periodo', StringType(), True)\n",
    "      ]\n",
    "    ))\n",
    "\n",
    "#  creamos una nueva columna: withColumn(\"un alias\", from_json(\"la columna a parsear\", el_schema_creado))\n",
    "mapped_df = df_json.withColumn(\"act\", when(df_json.actividad_regimen_general == 'Null', 'no')\n",
    "                               .otherwise(from_json(\"actividad_regimen_general\", schema))   )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "627cb5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+--------------+----+\n",
      "|actividad_regimen_general|partition_date| act|\n",
      "+-------------------------+--------------+----+\n",
      "|                     Null|    2023-05-01|null|\n",
      "+-------------------------+--------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mapped_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8830a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- actividad_regimen_general: string (nullable = true)\n",
      " |-- partition_date: string (nullable = true)\n",
      " |-- act: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- descripcionActividad: string (nullable = true)\n",
      " |    |    |-- idActividad: string (nullable = true)\n",
      " |    |    |-- nomenclador: string (nullable = true)\n",
      " |    |    |-- orden: string (nullable = true)\n",
      " |    |    |-- periodo: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mapped_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789624f4",
   "metadata": {},
   "source": [
    "## Desarmar la Lista\n",
    "\n",
    "Ahora con la funcion `explode` directamente al campo, creara un nuevo registro por cada elemento de la lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d56d0090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| act|\n",
      "+----+\n",
      "|null|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mapped_df.select(\"act\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ae75285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+--------------+---+-------+\n",
      "|actividad_regimen_general|partition_date|act|act_exp|\n",
      "+-------------------------+--------------+---+-------+\n",
      "+-------------------------+--------------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = mapped_df.withColumn(\"act_exp\", explode(\"act\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99096698",
   "metadata": {},
   "source": [
    "Para acceder a los campos dentro de las estructuras\n",
    "\n",
    "Usamos la notacion punto `(\"x.y\")`   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56880f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------+\n",
      "|descripcionActividad|orden| periodo|\n",
      "+--------------------+-----+--------+\n",
      "|    CULTIVO DE TRIGO|  3.0|201311.0|\n",
      "|     CULTIVO DE MAÍZ|  4.0|201311.0|\n",
      "|CULTIVO DE PASTOS...|  6.0|201311.0|\n",
      "|  CULTIVO DE GIRASOL|  5.0|201311.0|\n",
      "|CRÍA DE GANADO BO...|  1.0|201311.0|\n",
      "|INVERNADA  DE GAN...|  2.0|201311.0|\n",
      "|SERVICIOS INMOBIL...|  7.0|201405.0|\n",
      "+--------------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(\"act_exp.descripcionActividad\",\"act_exp.orden\",\"act_exp.periodo\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3398e178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-----------+-----+--------+\n",
      "|descripcionActividad|idActividad|nomenclador|orden| periodo|\n",
      "+--------------------+-----------+-----------+-----+--------+\n",
      "|    CULTIVO DE TRIGO|    11112.0|      883.0|  3.0|201311.0|\n",
      "|     CULTIVO DE MAÍZ|    11121.0|      883.0|  4.0|201311.0|\n",
      "|CULTIVO DE PASTOS...|    11130.0|      883.0|  6.0|201311.0|\n",
      "|  CULTIVO DE GIRASOL|    11291.0|      883.0|  5.0|201311.0|\n",
      "|CRÍA DE GANADO BO...|    14113.0|      883.0|  1.0|201311.0|\n",
      "|INVERNADA  DE GAN...|    14114.0|      883.0|  2.0|201311.0|\n",
      "|SERVICIOS INMOBIL...|   681098.0|      883.0|  7.0|201405.0|\n",
      "+--------------------+-----------+-----------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Si quieren todo pueden usar el siempre u confiable (*)\n",
    "df2.select(\"act_exp.*\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
